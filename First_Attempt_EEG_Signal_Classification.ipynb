{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.python.keras import layers\n",
    "from tensorflow.python.keras.layers import Dense, Activation, Dropout\n",
    "from tensorflow.python.keras.optimizers import adam_v2\n",
    "from tensorflow.python.keras.metrics import CategoricalAccuracy\n",
    "from tensorflow.python.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "focussed_day1 = pd.read_csv(\"C:\\EEG Attention State\\CSV Files\\Day 3 - Focussed.csv\")\n",
    "focussed_day1 = focussed_day1.dropna(axis=0)\n",
    "drowsy_day1 = pd.read_csv(\"C:\\EEG Attention State\\CSV Files\\Day 3 - Semi-Focussed or Drowsy.csv\")\n",
    "unfocussed_day1 = pd.read_csv(\"C:\\EEG Attention State\\CSV Files\\Day 3 - Unfocussed or Sleeping.csv\")\n",
    "drowsy_day1 = drowsy_day1.dropna(axis=0)\n",
    "unfocussed_day1 = unfocussed_day1.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AF3</th>\n",
       "      <th>F7</th>\n",
       "      <th>F3</th>\n",
       "      <th>FC5</th>\n",
       "      <th>T7</th>\n",
       "      <th>P7</th>\n",
       "      <th>O1</th>\n",
       "      <th>O2</th>\n",
       "      <th>P8</th>\n",
       "      <th>T8</th>\n",
       "      <th>FC6</th>\n",
       "      <th>F4</th>\n",
       "      <th>F8</th>\n",
       "      <th>AF4</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4440.512821</td>\n",
       "      <td>3804.102564</td>\n",
       "      <td>5210.769231</td>\n",
       "      <td>3829.230769</td>\n",
       "      <td>4030.256410</td>\n",
       "      <td>4490.769231</td>\n",
       "      <td>3862.051282</td>\n",
       "      <td>3511.794872</td>\n",
       "      <td>4321.025641</td>\n",
       "      <td>4304.615385</td>\n",
       "      <td>4141.025641</td>\n",
       "      <td>4282.564103</td>\n",
       "      <td>4333.846154</td>\n",
       "      <td>4024.102564</td>\n",
       "      <td>Focussed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4440.512821</td>\n",
       "      <td>3796.410256</td>\n",
       "      <td>5205.641026</td>\n",
       "      <td>3827.179487</td>\n",
       "      <td>4030.256410</td>\n",
       "      <td>4489.230769</td>\n",
       "      <td>3858.461538</td>\n",
       "      <td>3505.128205</td>\n",
       "      <td>4311.794872</td>\n",
       "      <td>4304.615385</td>\n",
       "      <td>4144.615385</td>\n",
       "      <td>4282.564103</td>\n",
       "      <td>4335.384615</td>\n",
       "      <td>4017.948718</td>\n",
       "      <td>Focussed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4440.512821</td>\n",
       "      <td>3798.974359</td>\n",
       "      <td>5207.179487</td>\n",
       "      <td>3827.692308</td>\n",
       "      <td>4030.256410</td>\n",
       "      <td>4489.230769</td>\n",
       "      <td>3865.128205</td>\n",
       "      <td>3511.794872</td>\n",
       "      <td>4313.846154</td>\n",
       "      <td>4304.615385</td>\n",
       "      <td>4148.717949</td>\n",
       "      <td>4282.564103</td>\n",
       "      <td>4334.871795</td>\n",
       "      <td>4018.974359</td>\n",
       "      <td>Focussed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4440.512821</td>\n",
       "      <td>3801.538462</td>\n",
       "      <td>5210.256410</td>\n",
       "      <td>3830.769231</td>\n",
       "      <td>4030.256410</td>\n",
       "      <td>4490.256410</td>\n",
       "      <td>3866.153846</td>\n",
       "      <td>3511.282051</td>\n",
       "      <td>4315.897436</td>\n",
       "      <td>4304.615385</td>\n",
       "      <td>4146.666667</td>\n",
       "      <td>4282.564103</td>\n",
       "      <td>4334.358974</td>\n",
       "      <td>4022.051282</td>\n",
       "      <td>Focussed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4440.512821</td>\n",
       "      <td>3799.487179</td>\n",
       "      <td>5210.256410</td>\n",
       "      <td>3831.794872</td>\n",
       "      <td>4030.256410</td>\n",
       "      <td>4490.256410</td>\n",
       "      <td>3865.641026</td>\n",
       "      <td>3506.153846</td>\n",
       "      <td>4311.282051</td>\n",
       "      <td>4304.615385</td>\n",
       "      <td>4145.641026</td>\n",
       "      <td>4282.564103</td>\n",
       "      <td>4333.846154</td>\n",
       "      <td>4020.512821</td>\n",
       "      <td>Focussed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929062</th>\n",
       "      <td>4452.820513</td>\n",
       "      <td>3960.512821</td>\n",
       "      <td>5045.641026</td>\n",
       "      <td>3844.615385</td>\n",
       "      <td>4045.641026</td>\n",
       "      <td>4325.128205</td>\n",
       "      <td>4213.846154</td>\n",
       "      <td>3943.589744</td>\n",
       "      <td>4123.589744</td>\n",
       "      <td>4309.743590</td>\n",
       "      <td>4137.435897</td>\n",
       "      <td>4283.076923</td>\n",
       "      <td>4336.923077</td>\n",
       "      <td>4101.025641</td>\n",
       "      <td>Unfocussed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929063</th>\n",
       "      <td>4455.897436</td>\n",
       "      <td>3956.410256</td>\n",
       "      <td>5051.794872</td>\n",
       "      <td>3843.589744</td>\n",
       "      <td>4043.076923</td>\n",
       "      <td>4324.615385</td>\n",
       "      <td>4214.871795</td>\n",
       "      <td>3942.051282</td>\n",
       "      <td>4120.512821</td>\n",
       "      <td>4307.179487</td>\n",
       "      <td>4144.615385</td>\n",
       "      <td>4286.153846</td>\n",
       "      <td>4335.384615</td>\n",
       "      <td>4096.923077</td>\n",
       "      <td>Unfocussed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929064</th>\n",
       "      <td>4444.102564</td>\n",
       "      <td>3960.512821</td>\n",
       "      <td>5041.025641</td>\n",
       "      <td>3831.282051</td>\n",
       "      <td>4042.564103</td>\n",
       "      <td>4323.589744</td>\n",
       "      <td>4214.871795</td>\n",
       "      <td>3949.743590</td>\n",
       "      <td>4122.564103</td>\n",
       "      <td>4309.743590</td>\n",
       "      <td>4149.743590</td>\n",
       "      <td>4287.179487</td>\n",
       "      <td>4334.871795</td>\n",
       "      <td>4096.923077</td>\n",
       "      <td>Unfocussed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929065</th>\n",
       "      <td>4438.461538</td>\n",
       "      <td>3959.487179</td>\n",
       "      <td>5031.794872</td>\n",
       "      <td>3834.358974</td>\n",
       "      <td>4042.051282</td>\n",
       "      <td>4323.076923</td>\n",
       "      <td>4216.923077</td>\n",
       "      <td>3956.410256</td>\n",
       "      <td>4124.615385</td>\n",
       "      <td>4317.435897</td>\n",
       "      <td>4155.384615</td>\n",
       "      <td>4287.692308</td>\n",
       "      <td>4334.871795</td>\n",
       "      <td>4095.384615</td>\n",
       "      <td>Unfocussed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>929066</th>\n",
       "      <td>4440.000000</td>\n",
       "      <td>3956.410256</td>\n",
       "      <td>5035.384615</td>\n",
       "      <td>3840.000000</td>\n",
       "      <td>4043.589744</td>\n",
       "      <td>4323.076923</td>\n",
       "      <td>4220.512821</td>\n",
       "      <td>3960.512821</td>\n",
       "      <td>4127.692308</td>\n",
       "      <td>4318.461538</td>\n",
       "      <td>4162.564103</td>\n",
       "      <td>4283.589744</td>\n",
       "      <td>4334.358974</td>\n",
       "      <td>4095.384615</td>\n",
       "      <td>Unfocussed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1543464 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                AF3           F7           F3          FC5           T7   \n",
       "0       4440.512821  3804.102564  5210.769231  3829.230769  4030.256410  \\\n",
       "1       4440.512821  3796.410256  5205.641026  3827.179487  4030.256410   \n",
       "2       4440.512821  3798.974359  5207.179487  3827.692308  4030.256410   \n",
       "3       4440.512821  3801.538462  5210.256410  3830.769231  4030.256410   \n",
       "4       4440.512821  3799.487179  5210.256410  3831.794872  4030.256410   \n",
       "...             ...          ...          ...          ...          ...   \n",
       "929062  4452.820513  3960.512821  5045.641026  3844.615385  4045.641026   \n",
       "929063  4455.897436  3956.410256  5051.794872  3843.589744  4043.076923   \n",
       "929064  4444.102564  3960.512821  5041.025641  3831.282051  4042.564103   \n",
       "929065  4438.461538  3959.487179  5031.794872  3834.358974  4042.051282   \n",
       "929066  4440.000000  3956.410256  5035.384615  3840.000000  4043.589744   \n",
       "\n",
       "                 P7           O1           O2           P8           T8   \n",
       "0       4490.769231  3862.051282  3511.794872  4321.025641  4304.615385  \\\n",
       "1       4489.230769  3858.461538  3505.128205  4311.794872  4304.615385   \n",
       "2       4489.230769  3865.128205  3511.794872  4313.846154  4304.615385   \n",
       "3       4490.256410  3866.153846  3511.282051  4315.897436  4304.615385   \n",
       "4       4490.256410  3865.641026  3506.153846  4311.282051  4304.615385   \n",
       "...             ...          ...          ...          ...          ...   \n",
       "929062  4325.128205  4213.846154  3943.589744  4123.589744  4309.743590   \n",
       "929063  4324.615385  4214.871795  3942.051282  4120.512821  4307.179487   \n",
       "929064  4323.589744  4214.871795  3949.743590  4122.564103  4309.743590   \n",
       "929065  4323.076923  4216.923077  3956.410256  4124.615385  4317.435897   \n",
       "929066  4323.076923  4220.512821  3960.512821  4127.692308  4318.461538   \n",
       "\n",
       "                FC6           F4           F8          AF4       Class  \n",
       "0       4141.025641  4282.564103  4333.846154  4024.102564    Focussed  \n",
       "1       4144.615385  4282.564103  4335.384615  4017.948718    Focussed  \n",
       "2       4148.717949  4282.564103  4334.871795  4018.974359    Focussed  \n",
       "3       4146.666667  4282.564103  4334.358974  4022.051282    Focussed  \n",
       "4       4145.641026  4282.564103  4333.846154  4020.512821    Focussed  \n",
       "...             ...          ...          ...          ...         ...  \n",
       "929062  4137.435897  4283.076923  4336.923077  4101.025641  Unfocussed  \n",
       "929063  4144.615385  4286.153846  4335.384615  4096.923077  Unfocussed  \n",
       "929064  4149.743590  4287.179487  4334.871795  4096.923077  Unfocussed  \n",
       "929065  4155.384615  4287.692308  4334.871795  4095.384615  Unfocussed  \n",
       "929066  4162.564103  4283.589744  4334.358974  4095.384615  Unfocussed  \n",
       "\n",
       "[1543464 rows x 15 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged = pd.concat([focussed_day1, drowsy_day1, unfocussed_day1])\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features = ['F7', 'F3', 'P7', 'O1', 'O2', 'P8', 'AF4']\n",
    "test_value_features = ['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#focussed_day2 = pd.read_csv(\"C:\\EEG Attention State\\CSV Files\\Day 4 - Focussed.csv\")\n",
    "#drowsy_day2 = pd.read_csv(\"C:\\EEG Attention State\\CSV Files\\Day 4 - Semi-Focussed or Drowsy.csv\")\n",
    "#unfocussed_day2 = pd.read_csv(\"C:\\EEG Attention State\\CSV Files\\Day 4 - Unfocussed or Sleeping.csv\")\n",
    "#focussed_day2 = focussed_day2.dropna(axis=0)\n",
    "#drowsy_day2 = drowsy_day2.dropna(axis=0)\n",
    "#unfocussed_day2 = unfocussed_day2.dropna(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merged2 = pd.concat([focussed_day2, drowsy_day2, unfocussed_day2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train1 = merged[test_features]\n",
    "y_train1 = merged[test_value_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_train1, y_train1, test_size=0.2, random_state=128, stratify=y_train1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_encoded = pd.get_dummies(y_train)\n",
    "y_test_encoded = pd.get_dummies(y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_val = merged2[test_features]\n",
    "#y_val = merged2[test_value_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=[7]))  \n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))  \n",
    "model.add(Dense(64))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(3)) \n",
    "model.add(Activation('softmax'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(),\n",
    "    metrics=['accuracy'],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "76/76 [==============================] - 3s 36ms/step - loss: 0.6653 - accuracy: 0.7193 - val_loss: 0.6090 - val_accuracy: 0.7450\n",
      "Epoch 2/50\n",
      "76/76 [==============================] - 2s 33ms/step - loss: 0.6648 - accuracy: 0.7191 - val_loss: 0.6076 - val_accuracy: 0.7459\n",
      "Epoch 3/50\n",
      "76/76 [==============================] - 2s 31ms/step - loss: 0.6640 - accuracy: 0.7198 - val_loss: 0.6069 - val_accuracy: 0.7461\n",
      "Epoch 4/50\n",
      "76/76 [==============================] - 2s 31ms/step - loss: 0.6654 - accuracy: 0.7192 - val_loss: 0.6069 - val_accuracy: 0.7464\n",
      "Epoch 5/50\n",
      "76/76 [==============================] - 2s 31ms/step - loss: 0.6642 - accuracy: 0.7198 - val_loss: 0.6068 - val_accuracy: 0.7459\n",
      "Epoch 6/50\n",
      "76/76 [==============================] - 2s 32ms/step - loss: 0.6639 - accuracy: 0.7202 - val_loss: 0.6072 - val_accuracy: 0.7462\n",
      "Epoch 7/50\n",
      "76/76 [==============================] - 3s 35ms/step - loss: 0.6632 - accuracy: 0.7200 - val_loss: 0.6061 - val_accuracy: 0.7469\n",
      "Epoch 8/50\n",
      "76/76 [==============================] - 3s 33ms/step - loss: 0.6637 - accuracy: 0.7200 - val_loss: 0.6059 - val_accuracy: 0.7462\n",
      "Epoch 9/50\n",
      "76/76 [==============================] - 2s 33ms/step - loss: 0.6634 - accuracy: 0.7200 - val_loss: 0.6052 - val_accuracy: 0.7460\n",
      "Epoch 10/50\n",
      "76/76 [==============================] - 3s 34ms/step - loss: 0.6634 - accuracy: 0.7201 - val_loss: 0.6048 - val_accuracy: 0.7472\n",
      "Epoch 11/50\n",
      "76/76 [==============================] - 3s 34ms/step - loss: 0.6633 - accuracy: 0.7203 - val_loss: 0.6049 - val_accuracy: 0.7469\n",
      "Epoch 12/50\n",
      "76/76 [==============================] - 3s 38ms/step - loss: 0.6629 - accuracy: 0.7206 - val_loss: 0.6047 - val_accuracy: 0.7476\n",
      "Epoch 13/50\n",
      "76/76 [==============================] - 3s 35ms/step - loss: 0.6631 - accuracy: 0.7202 - val_loss: 0.6053 - val_accuracy: 0.7475\n",
      "Epoch 14/50\n",
      "76/76 [==============================] - 3s 35ms/step - loss: 0.6625 - accuracy: 0.7204 - val_loss: 0.6034 - val_accuracy: 0.7485\n",
      "Epoch 15/50\n",
      "76/76 [==============================] - 3s 35ms/step - loss: 0.6627 - accuracy: 0.7206 - val_loss: 0.6041 - val_accuracy: 0.7481\n",
      "Epoch 16/50\n",
      "76/76 [==============================] - 2s 32ms/step - loss: 0.6623 - accuracy: 0.7206 - val_loss: 0.6045 - val_accuracy: 0.7480\n",
      "Epoch 17/50\n",
      "76/76 [==============================] - 3s 34ms/step - loss: 0.6623 - accuracy: 0.7209 - val_loss: 0.6032 - val_accuracy: 0.7488\n",
      "Epoch 18/50\n",
      "76/76 [==============================] - 2s 33ms/step - loss: 0.6621 - accuracy: 0.7202 - val_loss: 0.6027 - val_accuracy: 0.7489\n",
      "Epoch 19/50\n",
      "76/76 [==============================] - 2s 31ms/step - loss: 0.6618 - accuracy: 0.7207 - val_loss: 0.6035 - val_accuracy: 0.7469\n",
      "Epoch 20/50\n",
      "76/76 [==============================] - 2s 31ms/step - loss: 0.6622 - accuracy: 0.7206 - val_loss: 0.6032 - val_accuracy: 0.7484\n",
      "Epoch 21/50\n",
      "76/76 [==============================] - 2s 33ms/step - loss: 0.6618 - accuracy: 0.7208 - val_loss: 0.6034 - val_accuracy: 0.7477\n",
      "Epoch 22/50\n",
      "76/76 [==============================] - 3s 35ms/step - loss: 0.6610 - accuracy: 0.7213 - val_loss: 0.6031 - val_accuracy: 0.7487\n",
      "Epoch 23/50\n",
      "76/76 [==============================] - 2s 33ms/step - loss: 0.6612 - accuracy: 0.7213 - val_loss: 0.6028 - val_accuracy: 0.7489\n",
      "Epoch 24/50\n",
      "76/76 [==============================] - 3s 34ms/step - loss: 0.6615 - accuracy: 0.7212 - val_loss: 0.6022 - val_accuracy: 0.7487\n",
      "Epoch 25/50\n",
      "76/76 [==============================] - 3s 33ms/step - loss: 0.6605 - accuracy: 0.7220 - val_loss: 0.6017 - val_accuracy: 0.7487\n",
      "Epoch 26/50\n",
      "76/76 [==============================] - 3s 35ms/step - loss: 0.6617 - accuracy: 0.7212 - val_loss: 0.6015 - val_accuracy: 0.7487\n",
      "Epoch 27/50\n",
      "76/76 [==============================] - 3s 35ms/step - loss: 0.6612 - accuracy: 0.7218 - val_loss: 0.6024 - val_accuracy: 0.7486\n",
      "Epoch 28/50\n",
      "76/76 [==============================] - 3s 36ms/step - loss: 0.6609 - accuracy: 0.7212 - val_loss: 0.6024 - val_accuracy: 0.7481\n",
      "Epoch 29/50\n",
      "76/76 [==============================] - 3s 34ms/step - loss: 0.6608 - accuracy: 0.7212 - val_loss: 0.6013 - val_accuracy: 0.7489\n",
      "Epoch 30/50\n",
      "76/76 [==============================] - 2s 32ms/step - loss: 0.6602 - accuracy: 0.7215 - val_loss: 0.6013 - val_accuracy: 0.7494\n",
      "Epoch 31/50\n",
      "76/76 [==============================] - ETA: 0s - loss: 0.6607 - accuracy: 0.72 - 2s 32ms/step - loss: 0.6607 - accuracy: 0.7217 - val_loss: 0.6011 - val_accuracy: 0.7485\n",
      "Epoch 32/50\n",
      "76/76 [==============================] - 2s 32ms/step - loss: 0.6602 - accuracy: 0.7220 - val_loss: 0.6007 - val_accuracy: 0.7497\n",
      "Epoch 33/50\n",
      "76/76 [==============================] - 2s 32ms/step - loss: 0.6608 - accuracy: 0.7220 - val_loss: 0.6015 - val_accuracy: 0.7492\n",
      "Epoch 34/50\n",
      "76/76 [==============================] - 2s 32ms/step - loss: 0.6602 - accuracy: 0.7216 - val_loss: 0.6011 - val_accuracy: 0.7487\n",
      "Epoch 35/50\n",
      "76/76 [==============================] - 3s 33ms/step - loss: 0.6599 - accuracy: 0.7219 - val_loss: 0.6001 - val_accuracy: 0.7506\n",
      "Epoch 36/50\n",
      "76/76 [==============================] - 3s 34ms/step - loss: 0.6600 - accuracy: 0.7222 - val_loss: 0.6008 - val_accuracy: 0.7502\n",
      "Epoch 37/50\n",
      "76/76 [==============================] - 3s 35ms/step - loss: 0.6597 - accuracy: 0.7223 - val_loss: 0.6005 - val_accuracy: 0.7497\n",
      "Epoch 38/50\n",
      "76/76 [==============================] - 2s 33ms/step - loss: 0.6592 - accuracy: 0.7224 - val_loss: 0.6008 - val_accuracy: 0.7495\n",
      "Epoch 39/50\n",
      "76/76 [==============================] - 2s 33ms/step - loss: 0.6595 - accuracy: 0.7220 - val_loss: 0.6008 - val_accuracy: 0.7489\n",
      "Epoch 40/50\n",
      "76/76 [==============================] - 3s 33ms/step - loss: 0.6591 - accuracy: 0.7223 - val_loss: 0.6000 - val_accuracy: 0.7501\n",
      "Epoch 41/50\n",
      "76/76 [==============================] - 3s 34ms/step - loss: 0.6592 - accuracy: 0.7223 - val_loss: 0.6002 - val_accuracy: 0.7489\n",
      "Epoch 42/50\n",
      "76/76 [==============================] - 3s 33ms/step - loss: 0.6592 - accuracy: 0.7222 - val_loss: 0.6000 - val_accuracy: 0.7495\n",
      "Epoch 43/50\n",
      "76/76 [==============================] - 3s 33ms/step - loss: 0.6601 - accuracy: 0.7224 - val_loss: 0.6001 - val_accuracy: 0.7487\n",
      "Epoch 44/50\n",
      "76/76 [==============================] - 2s 31ms/step - loss: 0.6592 - accuracy: 0.7221 - val_loss: 0.6001 - val_accuracy: 0.7507\n",
      "Epoch 45/50\n",
      "76/76 [==============================] - 2s 32ms/step - loss: 0.6591 - accuracy: 0.7222 - val_loss: 0.5992 - val_accuracy: 0.7496\n",
      "Epoch 46/50\n",
      "76/76 [==============================] - 2s 31ms/step - loss: 0.6587 - accuracy: 0.7225 - val_loss: 0.5984 - val_accuracy: 0.7507\n",
      "Epoch 47/50\n",
      "76/76 [==============================] - 2s 32ms/step - loss: 0.6591 - accuracy: 0.7226 - val_loss: 0.6002 - val_accuracy: 0.7491\n",
      "Epoch 48/50\n",
      "76/76 [==============================] - 2s 30ms/step - loss: 0.6590 - accuracy: 0.7223 - val_loss: 0.6000 - val_accuracy: 0.7502\n",
      "Epoch 49/50\n",
      "76/76 [==============================] - 2s 31ms/step - loss: 0.6595 - accuracy: 0.7227 - val_loss: 0.6007 - val_accuracy: 0.7491\n",
      "Epoch 50/50\n",
      "76/76 [==============================] - 2s 31ms/step - loss: 0.6592 - accuracy: 0.7223 - val_loss: 0.6003 - val_accuracy: 0.7487\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train_scaled, y_train_encoded, batch_size=16384, epochs=50, validation_data = (X_val_scaled, y_test_encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(308693, 7)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X_val_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(308693, 1)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1234771, 7)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X_train_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1234771, 1)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1234771, 3)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(308693, 3)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_encoded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9647/9647 [==============================] - 12s 1ms/step - loss: 0.6076 - accuracy: 0.7456\n",
      "Test Accuracy: 0.7455757260322571\n",
      "Test Loss: 0.6076471209526062\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(X_val_scaled, y_test_encoded)\n",
    "\n",
    "print('Test Accuracy:', accuracy)\n",
    "print(\"Test Loss:\", loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ae4550330c66d1b951b113f9fe9833d7ce9ecc0ed4ad0781832fd4e49bd97bba"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
